# -*- coding: utf-8 -*-
"""Project 1: Multiple linear regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yVTNlGm2NlA86a8Hb6KxL0Fgn_NNv-5H

# **Project 1: Multiple linear regression**

## Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from scipy import stats

"""## Load the Dataset"""

df=pd.read_csv('/content/cars.csv')
df.shape

df.head()

df.info()

#numerical_variables
numerical_variables=df.select_dtypes(include=['int64','float64']).columns
numerical_variables

#categorical variables
categorical_variables=df.select_dtypes(include=['object']).columns
categorical_variables

df.describe()

"""## Check for missing values"""

print("Missing values in each column:\n", df.isnull().sum())

"""## Check for Duplicate rows"""

duplicates = df.duplicated().sum()
print("Number of duplicate rows:", duplicates)

# Remove duplicates if any
data = df.drop_duplicates()
print("New dataset dimensions after removing duplicates:", data.shape)

"""## Handle Categorical Variables (One-Hot Encoding)"""

data_encoded = pd.get_dummies(data, columns=categorical_variables, drop_first=True)

# Check the new dataset shape after encoding
print("Shape after encoding categorical variables:", data_encoded.shape)

target_column = 'Engine Information.Engine Statistics.Horsepower'
X = df.drop(columns=[target_column])  # Features
y = df[target_column]

"""## Detect Outliers Using IQR Method

"""

# Define a function to detect outliers using IQR
def detect_outliers(df):
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    return ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()

# Detect outliers
outliers = detect_outliers(df[numerical_variables])
print(outliers)

# Handle outliers using the IQR method
for col in numerical_variables:
    # Calculate the first quartile (Q1) and the third quartile (Q3)
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)

    # Calculate the interquartile range (IQR)
    IQR = Q3 - Q1

    # Define lower and upper bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter out the outliers
    data = data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]

# Display the shape of the dataset after removing outliers
print("Data shape after handling outliers:", data.shape)

"""## Data Visualization"""

# Correlation matrix
plt.figure(figsize=(10, 8))
corr_matrix = df[numerical_variables].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Distribution of target variable (City MPG)
sns.histplot(df['Fuel Information.City mpg'], kde=True)
plt.title('Distribution of City MPG')
plt.show()

"""## Feature Scaling(Standardization)"""

#Feature scaling
scaler=StandardScaler()
data_encoded[numerical_variables]=scaler.fit_transform(data_encoded[numerical_variables])
data_encoded.head()

# Define independent variables (X)
X = df[[
    "Dimensions.Height", "Dimensions.Length", "Dimensions.Width",
    "Engine Information.Number of Forward Gears", "Fuel Information.City mpg","Fuel Information.Highway mpg",
    "Engine Information.Engine Statistics.Torque"
]]

# Standardize the data (optional but improves VIF accuracy)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Compute VIF
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]

# Display VIF values
print("Variance Inflation Factor (VIF):\n", vif_data)

"""## Correlation Analysis and Multicollinearity Check for Feature Selection"""

# Check correlation between 'City mpg' and 'Highway mpg'
corr = df[["Fuel Information.City mpg", "Fuel Information.Highway mpg"]].corr()
print("Correlation between City mpg and Highway mpg:\n", corr)

# Drop the correlated feature 'Fuel Information.Highway mpg'
X = df[[
    "Dimensions.Height", "Dimensions.Length", "Dimensions.Width",
    "Engine Information.Number of Forward Gears", "Fuel Information.City mpg",
    "Engine Information.Engine Statistics.Torque"
]]

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Compute new VIF values
vif_data = pd.DataFrame()
vif_data["Feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]

print("Updated VIF values:\n", vif_data)

"""## Train-Test Split"""

# Define the independent (X) and dependent (y) variables
X = data_encoded.drop('Fuel Information.City mpg', axis=1)  # Dropping the target variable 'City mpg'
y = data_encoded['Fuel Information.City mpg']  # The target variable
X_scaled = scaler.fit_transform(X)
# Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""## Build and Train the Regression Model"""

# Train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Lasso Regression
lasso_model = Lasso(alpha=0.1)  # Experiment with different alpha values
lasso_model.fit(X_train, y_train)

"""## Model Evaluation"""

# Make predictions on the training and test sets
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

# Evaluate the model
print('Training R^2:', r2_score(y_train, y_pred_train))
print('Test R^2:', r2_score(y_test, y_pred_test))
print('Training RMSE:', np.sqrt(mean_squared_error(y_train, y_pred_train)))
print('Test RMSE:', np.sqrt(mean_squared_error(y_test, y_pred_test)))

# Actual vs Predicted plot
plt.scatter(y_test, y_pred_test)
plt.title('Actual vs Predicted City MPG')
plt.xlabel('Actual values')
plt.ylabel('Predicted values')
plt.show()

"""## Residual Diagnostics

### Residuals vs Fitted Plot (Homoscedasticity Check):
"""

# Residual plot (homoscedasticity check)
residuals = y_train - y_pred_train
sns.residplot(x=y_pred_train, y=residuals)
plt.title('Residuals vs Fitted')
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
plt.show()

"""### Q-Q Plot (Normality Check)"""

# Q-Q plot for residuals
sm.qqplot(residuals, line='s')
plt.title('Q-Q Plot for Residuals')
plt.show()

"""### Durbin-Watson Test (Independence of Residuals)"""

# Durbin-Watson test for autocorrelation in residuals
from statsmodels.stats.stattools import durbin_watson
dw = durbin_watson(residuals)
print('Durbin-Watson:', dw)